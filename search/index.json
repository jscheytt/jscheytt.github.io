[{"content":"I have a dotfiles repository for setting up my MacOS machine easily and reproducibly. For this I am mostly using Ansible, just slightly wrapping it with Make and seasoning it with a pinch of Shell.\nThe core piece of this repository is a Brewfile and a collection of Shell aliases and functions. I don\u0026rsquo;t regularly run the Ansible playbook itself because I created it for initially setting up a machine. The only thing I run very frequently is the upgrade command which includes persisting new Brew formulae to the Brewfile.\nAt the same time I have come to love applying GitOps principles to everything I work with. Just the other day I was moving some commands (in the vein of \u0026ldquo;global Git pull\u0026rdquo;) out of said upgrade command into a cronjob.\nAnd then it hit me: If I create a cronjob for running my Ansible playbook through the playbook itself, I have GitOps-like reconciliation!\nLet\u0026rsquo;s do it in just a few lines of yaml:\n- name:Ensure dotfiles are appliedcron:name:Ensure dotfiles are appliedminute:\u0026#34;0\u0026#34;hour:\u0026#34;9\u0026#34;weekday:\u0026#34;1-5\u0026#34;# on workdaysjob:make -f \u0026#34;{{ ansible_env.PWD }}\u0026#34;/Makefile build","date":"2022-03-01T08:34:20+01:00","permalink":"https://jscheytt.github.io/p/you-can-gitopsify-your-ansible/","title":"You can Gitopsify your Ansible"},{"content":"Recently I wanted to do a bulk cleanup on some GitHub repositories I am responsible for, deleting old branches that have already been merged into the default branches. I first considered performing it through the GitHub API, but then I decided to try doing it via Git itself.\nAfter I had begun dabbling with a few wrapper scripts, I suddenly remembered something which massively simplified my strategy: Git Aliases. These are Git commands you can define yourself, either via CLI or in the Gitconfig file.\nWith this article, I want to introduce what I learned about Git aliases – and in the process, you get all the aliases I defined for my cleanup 😉\nDefining Shortcuts Many articles about Git aliases explain only the shortcut side. They show e.g. how you can abbreviate git checkout to git co by running git config --global alias.co checkout. Alternatively to the CLI command, you can add this section to your ~/.gitconfig file:\n[alias] co = checkout Nowadays, with the Git plugin of oh-my-zsh, I don\u0026rsquo;t feel there is a great need for such shortcuts. Let\u0026rsquo;s instead talk about actual custom commands:\nWith Parameters If you use an exclamation mark before your command, you can run any Shell command you want, even with parameters. The following example will let you do e.g. git cat 2eea778 package.json to get the file contents of a file at a certain revision:\n[alias] ; Output file contents from any revision ; See https://stackoverflow.com/a/54819889/6435726 cat = !git show \u0026#34;$1:$2\u0026#34; Pass It On Piping output into other commands is available out of the box. Executing multiple commands is just a \u0026amp;\u0026amp; away.\nYou may want to break your command into multiple lines: Do so by wrapping your command into quotes and prepending every new line with a backslash.\n; What is the default branch of this repo? ; The first command asks the remote if the default branch was changed. default-branch = \u0026#34;! \\ git remote set-head origin -a \u0026gt; /dev/null \\ \u0026amp;\u0026amp; git rev-parse --abbrev-ref origin/HEAD \\ | sed \u0026#39;s#origin/##\u0026#39;\u0026#34; You can also use subshells:\n; Switch to the default branch. switch-default = !git switch $(git default-branch) Escaping Can Be Tricky If you want to have a literal backslash in the resulting Shell command, you have to escape it. Pay attention to the grep patterns in the following aliases: Every double backslash of this pattern becomes a single backslash when Git passes the command to the Shell.\n; Which branches have been merged into the default branch on the remote? ; For safety, manually add names of long-lived branches to the grep pattern. remotely-merged-branches = \u0026#34;! \\ git branch --all --merged $(git default-branch) \\ | { grep -vE \u0026#39;^\\\\*|(\\\\b($(git default-branch)|develop|main|master|quality)\\\\b)\u0026#39; || true; } \\ | sed \u0026#39;s#remotes/origin/##\u0026#39;\u0026#34; ; Which local branches are not present on the remote (but were once)? ; NOTE: `git remote prune origin` only deletes local snapshots ; of remote branches that were deleted on the remote. ; See https://stackoverflow.com/a/48820687/6435726 ; It will not delete local branches where the remote branch is \u0026#34;gone\u0026#34;. ; This command finds these local branches. local-branches-without-remote = \u0026#34;! \\ git remote prune origin \u0026amp;\u0026amp; \\ git branch --list --format \u0026#39;%(if:equals=[gone])%(upstream:track)%(then)%(refname)%(end)\u0026#39; \\ | awk NF \\ | sed \u0026#39;s#refs/heads/##\u0026#39;\u0026#34; I think these are the dangers of every templating language: You have to account for special characters - but if these special characters happen to be special in someone else\u0026rsquo;s language, things can become unexpectedly complicated. (Think about Makefiles and $(variables) vs. $$variables in rules.)\nParameters Pt. 2: Default Values As with any other Shell function, you can not only have positional parameters but you can also give them default values. The following alias has a delete flag that defaults to the safe behavior, but you can overwrite it with git delete-local-branches-without-remote -D:\n; Delete local branches that are not present on the remote ; (safely, including warnings). ; You can ignore the warnings by passing \u0026#34;-D\u0026#34; as a parameter. ; NOTE: `git remote prune origin` only deletes local snapshots ; of remote branches that were deleted on the remote. ; It will not delete local branches where the remote branch is \u0026#34;gone\u0026#34;. delete-local-branches-without-remote = \u0026#34;! \\ git local-branches-without-remote \\ | xargs -I {} git branch ${1:-\u0026#39;-d\u0026#39;} {}\u0026#34; This last alias is what finally deletes the remote branches I wanted to target. It also demonstrates nicely how you can use xargs to run every Shell command as if it was capable of handling stdin natively:\n; Delete branches on the remote which were merged. push-delete-remotely-merged-branches = \u0026#34;! \\ git switch-default \u0026amp;\u0026amp; \\ git remotely-merged-branches \\ | xargs -I {} git push origin --delete {}\u0026#34; Debugging If you encounter an error message, you can increase the verbosity with this environment variable:\nexport GIT_TRACE=1 Deactivate it afterward by closing your terminal session or explicitly with unset GIT_TRACE.\nBringing It All Together: Multiple Repositories As a developer, chances are high you have more than just one Git repository on your machine. For many everyday use cases (like keeping all your local clones up-to-date), I have been using git-repo-updater with a lot of success and ease.\nBut now I discovered I can use it to execute arbitrary commands (and also Git aliases 😉) in multiple Git repos. With the following Shell function I am wrapping gitup for convenience:\n# Execute a Git command on all Git repositories # $1: Path with Git repositories in subdirectories # Rest of parameters: Git command (e.g. \u0026#34;status\u0026#34;) function git-xargs() { local filepath=\u0026#34;$1\u0026#34; # shellcheck disable=SC2116 gitup --depth -1 \u0026#34;$filepath\u0026#34; --exec \u0026#34;git $(echo \u0026#34;${@:2}\u0026#34;)\u0026#34; } And now I can finally clean up all branches with just one command (and quite pretty output):\ngit-xargs ~/Documents push-delete-remotely-merged-branches git-xargs ~/Documents delete-local-branches-without-remote ","date":"2022-02-28T13:15:29+01:00","permalink":"https://jscheytt.github.io/p/the-git-commands-you-wish-you-always-had/","title":"The Git Commands You Wish You Always Had"},{"content":"$ kubectl explain statefulset KIND: StatefulSet VERSION: apps/v1 DESCRIPTION: StatefulSet represents a set of pods with consistent identities. Identities are defined as: - Network: A single stable DNS and hostname. - Storage: As many VolumeClaims as requested. The StatefulSet guarantees that a given network identity will always map to the same storage identity. FIELDS: apiVersion\t\u0026lt;string\u0026gt; APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources kind\t\u0026lt;string\u0026gt; Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata\t\u0026lt;Object\u0026gt; spec\t\u0026lt;Object\u0026gt; Spec defines the desired identities of pods in this set. status\t\u0026lt;Object\u0026gt; Status is the current status of Pods in this StatefulSet. This data may be out of date by some window of time. I guess this will reduce a lot of my googling in future.\nAnd it even works on nested fields!\n$ kubectl explain deployment.spec.template.spec.containers KIND: Deployment VERSION: apps/v1 RESOURCE: containers \u0026lt;[]Object\u0026gt; DESCRIPTION: List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated. A single application container that you want to run within a pod. FIELDS: args\t\u0026lt;[]string\u0026gt; Arguments to the entrypoint. The docker image\u0026#39;s CMD is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container\u0026#39;s environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell command\t\u0026lt;[]string\u0026gt; Entrypoint array. Not executed within a shell. The docker image\u0026#39;s ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container\u0026#39;s environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be updated. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell env\t\u0026lt;[]Object\u0026gt; List of environment variables to set in the container. Cannot be updated. ... ","date":"2022-02-01T14:53:18+01:00","permalink":"https://jscheytt.github.io/p/kubectl-can-explain-all-resource-types-in-detail/","title":"kubectl can explain all resource types in detail"},{"content":"Today I was refactoring a bigger configuration setup that is built with kustomize. I see kustomize as a light-weight way of packaging multiple Kubernetes manifests, together with a little bit of logic.\nThe base kustomization of the application config repo I was refactoring looked something like this:\napiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationresources:- manifests/cronjob-backup.yaml- manifests/deployment.yaml- manifests/ingress.yaml- manifests/pod-disruption-budget.yaml- manifests/service.yamlcommonAnnotations:source:javatenant:acmeconfigMapGenerator:- name:acme-app-configfiles:- configs/10-local.properties- configs/30-local.properties- configs/40-local.propertiesI have an almost pathologic tendency to simplify and DRY everything up that I find, especially in configuration code. And as I saw a lot of repeating Labels in the manifests, I thought \u0026ldquo;Well, let\u0026rsquo;s just unify them.\u0026rdquo;\nSo I went ahead, removed the repetitive Labels from the manifests and added the following block to the kustomization:\ncommonLabels:app:acme-appI knew a bit about Kubernetes Services and that they use Labels to find the Pods to which they should direct their traffic. That\u0026rsquo;s why I thoroughly verified that after my change the Service and the Deployment would still have the same selector labels. 🤗 Nothing seemed off \u0026hellip;\nI committed and pushed my changes, and after the Deployment had finished restarting, I clicked through the application. 👀 Oddly enough some of the requests succeeded as expected, but some kept failing with a 💥 502 Bad Gateway error! At first I tried troubleshooting quickly, but soon I opted for just reverting my changes and pushing the revert commits to undo my changes.\nFor debugging I compared the output of kustomize build before and after my changes. After some scrolling I came across the CronJobs I defined for backup1.\nI should probably not have been surprised that the CronJob also had the same Labels I gave it via the kustomization. But now a suspicion started sneaking in:\n What if the completed Pods of the CronJob received traffic from the Service because they had the same Labels?\n Following this idea, I refactored my configs a bit. Soon, I was able to ensure that requests to the Service would only point to my target Deployment (and not to any other Pods):\ndiff --git a/base/kustomization.yaml b/base/kustomization.yaml index 6c1c6f7..b77b0fc 100644 --- a/base/kustomization.yaml +++ b/base/kustomization.yaml @@ -15,7 +15,6 @@ commonAnnotations:  source: java tenant: acme -commonLabels: - app: acme-app  configMapGenerator: - name: acme-app-config diff --git a/base/manifests/deployment.yaml b/base/manifests/deployment.yaml index 1e4aa3f..a60a798 100644 --- a/base/manifests/deployment.yaml +++ b/base/manifests/deployment.yaml @@ -16,6 +16,9 @@ metadata:  spec: replicas: 1 + selector: + matchLabels: + app: acme-app  template: metadata: labels: + app: acme-app  spec: containers: diff --git a/base/manifests/service.yaml b/base/manifests/service.yaml index b05d898..89af021 100644 --- a/base/manifests/service.yaml +++ b/base/manifests/service.yaml @@ -10,3 +10,5 @@ spec:  - name: api port: 8080 + selector: + app: acme-app (Notice that the Deployment needs the Label both in .spec.selector.matchLabels and .spec.template.metadata.labels!)\nAnd surely enough, after deploying this fix, the request to the application worked flawlessly 😊✅.\n  It does not matter for which purpose I created this CronJob, it could have been any Kubernetes Resource that creates Pods.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"2021-12-03T10:43:01+01:00","permalink":"https://jscheytt.github.io/p/trying-to-simplify-k8s-labels-can-be-dangerous-for-your-routing/","title":"Trying to simplify k8s labels can be dangerous for your routing"},{"content":"kubectl api-resources --verbs=list --namespaced -o name \\  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n \u0026#34;$NAMESPACE\u0026#34; Props to RedHat.\n","date":"2021-10-29T12:11:47+02:00","permalink":"https://jscheytt.github.io/p/kubernetes-is-still-willing-to-show-you-dangling-resources-in-your-terminating-namespace/","title":"Kubernetes is still willing to show you Dangling Resources in your Terminating Namespace"},{"content":"UUIDs are 128-bit numbers. That means they have $2^{128}$ possible values which is roughly $3 \\cdot 10^{38}$ (or in scientific notation, 3e38).\nDoes this range make it safe for cryptographic purposes? Most people on the internet say an emphatic \u0026ldquo;No!\u0026rdquo;, so I am just keeping this here as a 🚨 disclaimer.\nBut is it at least collision-safe? I think definitely yes. And is it also guess-safe? Let\u0026rsquo;s explore that question with a bit of maths:\nExample Scenario Let\u0026rsquo;s say you have an API and you know the records are referenced by their UUID. And let\u0026rsquo;s say you wanted to guess any valid record (because 💰 money).\nWhat is a realistic scenario, i.e. a reasonably expectable time it would take you to find a valid record? Let\u0026rsquo;s just naively assume you find a valid UUID after randomly iterating over half of all possible UUIDs.\nHow long would that take you? Imagine you had started firing requests at your hypothetical API at the beginning of the observable universe, i.e. about 1.3772e10 years (or 4.3437e17 seconds) ago. And you send requests at a rate of 1 trillion (1e12) requests per second (and, of course, your target API responds at the same rate 😉). (You would have to be very physically close to the API server - at a trillionth of a second, light and therefore information travels only a measly 0.3 millimeters \u0026hellip;)\nHow many UUIDs would you have covered? You would have processed barely 1 billionth of all possible UUIDs (to be precise: 1e12 * 4,3437e17 / 2^128 ≈ 1.2765e-9). That is not even remotely close to half of all UUIDs. To cover half of all possible UUIDs you would have to continue for another \u0026hellip; how many years?\n$$ \\frac{(1.2765 \\cdot 10^{9} - 1) \\cdot 4.3437 \\cdot 10^{17} \\ seconds}{2} = 2.7725 \\cdot 10^{26} \\ seconds = 8.786 \\cdot 10^{18} \\ years $$\nAnd these roughly 10 quintillion years are roughly 1 billion times the age of the universe. So you would very probably never live to see the matching of the UUID.\nBut, just hypothetically assuming you have started your client\u0026rsquo;s requests at the dawn of time and can continue to run it on this Earth unattended for as long as this planet exists: Would you still physically have enough time to execute your requests (for half of all UUIDs)?\nWe are going into lots of speculation now, but:\n In latest 2e9 years, all life will have vanished from Earth, the oceans have evaporated, the surface temperature will reach around 150 ° C.  You better build your client and API servers very temperature robust 🏜 \u0026hellip;   In 1e14 years, all stars will have exhausted their fuel. Assuming the Earth has not been completely engulfed by the Sun during its red giant phase,  and assuming the Earth was not already ejected from its orbit into outer space (not another galaxy), then the Earth would surely collide with the then black dwarf Sun in 10e20 years.    So yes, with a bit of preparation (temperature hardening, robust solar panels, radiation shielding) your client server would probably finish just in time before the Earth itself becomes mere history 😉.\n","date":"2021-10-06T08:32:23+02:00","permalink":"https://jscheytt.github.io/p/guessing-uuids-would-actually-take-very-long/","title":"Guessing UUIDs would actually take very long"},{"content":"This year I have switched from an Android smartphone (Samsung Galaxy J3 (2016)) to an iPhone (SE 2020).\nWhen switching, these things stood out to me:\n There is no really usable calendar app.  I was using Business Calendar before and I still miss the month widget. I have settled with Google Calendar for now.   The swipe gestures from all possible sides of the screen instead of the 1-dimensional top drawer menu are really interesting, and I have grown very fond of them. The emergency contact feature is well thought out. Migrating WhatsApp chats is only possible via paid 3rd party apps.  I paid 30 USD for Dr. Fone, connecting both devices to a Desktop/Laptop computer, and it worked flawlessly (missing only the read/unread markers).   The mobile internet usage from the stock system is higher than on Android and I have no way reducing it. It used up 17 out of my precious 100 MB monthly quota for system \u0026ldquo;software updates\u0026rdquo; although I deactivated that on the App Store. Privacy is probably a lot better than on Android. See this article. Makes sense as Apple is a hardware company and Google is a data company. I have a lot more space on my phone now and I don\u0026rsquo;t have to worry about deleting apps for space. Many of the default apps and settings work really well for me - just I expected also from the MacOS experience.  But I deactivated all Office-ish apps   I am extremely disappointed and shocked that there is no solution (not even a paid one) to activate Do Not Disturb (DND) during calendar events.  I used a small Android app called Polite before and there is no equivalent for it, not even by a long shot!   Automatic dark/light mode depending on daytime is very nice.  And some iPhone SE-specific things I appreciate:\n Fingerprint/TouchID was a very good choice. It\u0026rsquo;s kind of sad though that you can add only up to 3 fingerprints (just like on MacOS). The size is a great relief as it\u0026rsquo;s smaller than most gigantic smartphones from basically all brands nowadays. The camera is really good even though it\u0026rsquo;s just one lens.  ","date":"2021-09-25T17:13:18+02:00","permalink":"https://jscheytt.github.io/p/transitioning-from-android-to-ios-the-good-the-bad-and-the-ugly/","title":"Transitioning from Android to iOS — The Good, the Bad, and the Ugly"},{"content":"As a CloudOps Engineer one key skill is automating repetitive tasks. What most people grab for intuitively is writing Shell scripts (be it Bash, zsh, fish or whichever flavor you prefer). And there are a lot many good reasons to do so:\n It is closest to typing commands directly in the terminal. You don\u0026rsquo;t have to learn a dedicated programming language. It is very portable to other platforms like e.g. a CI server.  But once you start managing an increasing number of tasks with your scripts, you start to face another problem: How do you manage your scripts?\nPersonally, I have always loved being able to enter some new place where conventions were already in place. It takes away so much work and mental effort at the beginning, and you can just get to work quickly. (That might explain why I fell in love with ❤️ Ruby on Rails before I dug into 💎 Ruby.)\nShell scripts by their very nature do not pose any restrictions regarding e.g. naming patterns or directory structures. Honestly, I think there never will be, and that is ok. But what I have come to appreciate a lot recently is Make as a companion for my Shell scripts.\n🏗 Make vs. 🐚 Shell in a 🥜 Nutshell  Purpose: Make is good at creating files, Shell is good at executing scripts. Portability: If your system has Bash, chances are pretty high that Make is also available. Developer API: Make has a clear entrypoint (namely make), Shell can be everything you want it to be.  Make is a tool that has its origin in the world of compiled languages, especially C. Compiling source code into binary artifacts (and doing so 🏎 economically) is what Make was originally designed for. I mean, the name of a tool should make its use clear, but let me just state this again for my future self: Make is meant to 🏗 make (create) files.\nIt\u0026rsquo;s all about target files. That\u0026rsquo;s why it makes sense to approach a Makefile with a mindset of \u0026ldquo;What do I want to create/build?\u0026rdquo; instead of \u0026ldquo;What do I want to perform?\u0026rdquo; To me this sounds very reminiscent of the distinction between declarative and imperative programming.\nSafely Versioned Secrets Management My concrete entrypoint into Make was the following use case I had lately, and it hopefully helps to illustrate the point of target files:\n ☸️ You have 2 AWS accounts with 1 Kubernetes cluster each. (One is for running a dev and a staging environment, the other one is running the production environment.) 🔑 Secrets are stored in AWS Secrets Manager and synced into the cluster via ExternalSecrets. ⛔️ You are not allowed to store secrets in Git, not even in encrypted form. 🪣 The secrets are JSON files and the key names are important, so you want to store them in Git.  What I did as a first step was to create sample secrets files that contained the keys but no valid data (kind of the schema of the secrets):\nsecrets ├── dev # One directory per target environment │ └── .keep ├── staging │ └── .keep ├── prod │ └── .keep ├── config1.sample.json # One sample file per secret ├── service2.sample.json └── service3.sample.json I put the keys I needed into the sample files and as a value a description of what to put in (or e.g. from which Password Service to fetch the value from). The file service2.sample.json would e.g. look like this.\n{ \u0026#34;EXTERNAL_API_KEY\u0026#34;: \u0026#34;API Key of EXTERNAL_SERVICE\u0026#34;, \u0026#34;CLOUD_SERVICE_CLIENT_SECRET\u0026#34;: \u0026#34;client secret for accessing CLOUD_SERVICE\u0026#34;, \u0026#34;CLOUD_SERVICE_PASSWORD\u0026#34;: \u0026#34;password for accessing CLOUD_SERVICE\u0026#34;, \u0026#34;BASIC_AUTH_PASSWORD\u0026#34;: \u0026#34;password for sending via Basic Auth\u0026#34; } The target structure I wanted to achieve was this:\nsecrets ├── dev │ ├── config1.json # This file contains the *keys* of │ │ # secrets/config1.sample.json │ │ # and the actual secret *values*! │ ├── service2.json │ ├── service3.json │ └── .keep ├── staging │ ├── config1.json # Contains key of sample file and │ │ # values for staging environment. │ ├── service2.json │ ├── service3.json │ └── .keep ├── prod │ ├── config1.json │ ├── service2.json │ ├── service3.json │ └── .keep ├── config1.sample.json ├── service2.sample.json └── service3.sample.json In order to not commit any actual secrets into version control, I added the following entries to my .gitignore:\n# Ignore secret data ... secrets/**/*.json # ... but keep the samples !secrets/*.sample.json Copying the Samples Now how do you copy the files to all environment\u0026rsquo;s directories? And how do you make sure you copy them exactly once (so you don\u0026rsquo;t lose the secrets you already entered)?\nYou could create a script with the following logic:\n# For each environment directory: ## For each sample file: ### Extract the service name ### Check if target secret file already exists ### If not, copy sample to target file But now, 🏗 Make to the rescue:\n### Variables  # Fetch all sample files. secrets_samples := $(wildcard secrets/*.sample.json) # Construct the paths for all dev secrets destinations. dev_secrets := $(patsubst secrets/%.sample.json,secrets/dev/%.json,$(secrets_samples)) # Construct the paths for all staging secrets destinations. staging_secrets := $(patsubst secrets/dev/%,secrets/staging/%,$(dev_secrets)) # Construct the paths for all staging secrets destinations. prod_secrets := $(patsubst secrets/dev/%,secrets/prod/%,$(dev_secrets)) # Gather the paths of all secrets\u0026#39; destinations. all_secrets := $(dev_secrets) $(staging_secrets) $(prod_secrets) ### Rules  # 🎯 Purpose: \u0026#34;Copy all samples to their destinations.\u0026#34; # 🤓 What Make sees: \u0026#34;When you build the file secrets.copy-templates, # make sure that all files in $(all_secrets) have been built first.\u0026#34; # 👩‍🏫 Explanation: A rule can be empty, and a rule can have prerequisites # on the first line. I like to think of such a rule as a kind of shortcut. secrets.copy-templates: $(all_secrets) # 🎯 Purpose: \u0026#34;Ensure that Make still runs the job \u0026#39;secrets.copy-templates\u0026#39; # even if a file called \u0026#39;secrets.copy-templates\u0026#39; is created.\u0026#34; # 🤓 What Make sees: \u0026#34;I am supposed to always build secrets.copy-templates # even if that file already exists.\u0026#34; .PHONY: secrets.copy-templates # 🎯 Purpose: \u0026#34;Copy the file on the right to the file on the left.\u0026#34; # 🤓 What Make sees: \u0026#34;When a file matching the pattern secrets/dev/(.*).json # is built, execute this rule. # Also first make sure that the corresponding file secrets/$1.sample.json # has been built before. # And the rule is: Copy the source file on the right ($\u0026lt;) to the destination # file on the left ($@).\u0026#34; # 👩‍🏫 Explanation: These 3 rules are applied when you call secrets.copy-templates # because it requires $(all_secrets) to be built. secrets/dev/%.json: secrets/%.sample.json cp $\u0026lt; $@ secrets/staging/%.json: secrets/%.sample.json cp $\u0026lt; $@ secrets/prod/%.json: secrets/%.sample.json cp $\u0026lt; $@ If you now execute make secrets.copy-templates, the sample files will be copied to all environment directories. And if you run that same command again, 🙊 Make will not copy anything because it intelligenty detected that the source files have not changed since the last execution.\nThe code above is certainly not optimal - I bet you could abstract away the environment names with bit of metaprogramming, but let\u0026rsquo;s not optimize prematurely. I think the result is already impressive, especially if you consider the following:\n ☝️ You don\u0026rsquo;t even have to call the job explicitly to run it. As long as secrets.copy-templates is the first build defined in the Makefile, you can even execute just make without any parameters. 👨‍💻 Onboarding a new colleague to your repository now sounds a lot more like: \u0026ldquo;Yes, do read the README, but above all execute make.\u0026rdquo;  ⛑ This is especially true if your Makefile contains good help texts for every rule.    How Not To Shoot Yourself in the Foot Make was made primarily for building binaries from source code. The fact that we are able to use it in the way described above comes with a warning: If you do the following, you will lose the secret data you already entered into the secret files:\n Execute make secrets.copy-templates. Edit a sample file. Execute make secrets.copy-templates. 💥 Make will copy and overwrite the edited sample file to all environment secret files.  Why? Make compares timestamps, and when the source has a newer last-modified timestamp than the destination it will execute the rule\nCan we circumvent this? We sure can. You can either make sure that you edit each environment file after editing the sample file. Or you change the last-modified timestamp via a build in the Makefile 😉:\nsecrets.ensure-copy-once: for f in $(all_secrets); do [ -f $$f ] \u0026amp;\u0026amp; touch $$f; done Now whenever you edit a sample file after the initial secrets.copy-templates you run this build via make secrets.ensure-copy-once and 🛡 your secrets will not be deleted.\nExtension: Environment-specific Sample Files One implicit assumption in my structure was that the secrets in service2 will always have the same schema in every environment. One day it so happened that service2 needed to have additional keys on prod, but they should not be present on dev or staging.\nI adjusted my desired structure like this:\nsecrets ├── dev │ ├── config1.json │ ├── service2.json # Contains keys from service2.sample.json │ ├── service3.json │ └── .keep ├── staging │ ├── config1.json │ ├── service2.json # Contains keys from service2.sample.json │ ├── service3.json │ └── .keep ├── prod │ ├── config1.json │ ├── service2.json # Contains keys from service2.sample.prod.json │ ├── service3.json │ └── .keep ├── config1.sample.json ├── service2.sample.json # Default sample file ├── service2.sample.prod.json # Prod-specific sample file └── service3.sample.json And I wrote my first Makefile function:\n# A Make function can take in an arbitrary number of numbered parameters. define copy_template cp $(1) $(2) @# Check if there is a more environment-specific sample file \t$(eval ENVIRONMENT := $(shell echo $(2) | sed -E \u0026#39;s#secrets/(.*)/.*#\\1#\u0026#39;)) $(eval ENVIRONMENT_SAMPLE_FILE := $(patsubst %.sample.json,%.sample.$(ENVIRONMENT).json,$(1))) @# If environment-specific file exists, copy it to destination if [ -f \u0026#34;$(ENVIRONMENT_SAMPLE_FILE)\u0026#34; ]; then cp $(ENVIRONMENT_SAMPLE_FILE) $(2); fi endef # Calling a Make function works by executing \u0026#39;call\u0026#39; # with the function name and all its parameters as a list. # My previous rules now became this: secrets/dev/%.json: secrets/%.sample.json $(call copy_template,$\u0026lt;,$@) secrets/staging/%.json: secrets/%.sample.json $(call copy_template,$\u0026lt;,$@) secrets/prod/%.json: secrets/%.sample.json $(call copy_template,$\u0026lt;,$@) 🍣 Perfect Symphony: Calling Scripts From Make It\u0026rsquo;s all good and nice to have your secrets created, but how do you deploy them to AWS Secrets Manager? Of course you write a thin wrapper around the wonderfully verbose AWS CLI:\n#!/usr/bin/env bash set -euo pipefail # set -x # DEBUG secret_name=\u0026#34;$1\u0026#34; # Use second argument or read stdin secret_value=\u0026#34;${2:-$(cat -)}\u0026#34; echo \u0026#34;$secret_value\u0026#34; # DEBUG # Create secret in idempotent way, avoid script from failing set +e aws secretsmanager create-secret --name \u0026#34;$secret_name\u0026#34; \u0026amp;\u0026gt; /dev/null set -e # Put secret value and output response to stdout aws secretsmanager put-secret-value --secret-id \u0026#34;$secret_name\u0026#34; \\  --secret-string \u0026#34;$secret_value\u0026#34; | cat In your terminal you would call it e.g. like this:\n./helpers/deploy-secret.sh envs/dev/config1-secrets \u0026lt; secrets/dev/config1.json\nLet\u0026rsquo;s make a generic rule in Make to execute this script:\n# The dependency on $(all_secrets) is to make sure that the secrets files exist # before deploying them. secret.deploy: $(all_secrets) ./helpers/deploy-secret.sh $(name) \u0026lt; secrets/$(environment)/$(filename) The call to the script that you executed above would become this:\nmake secret.deploy name=envs/dev/config1-secrets environment=dev filename=config1\nAs we have multiple services, let\u0026rsquo;s add one rule per service:\n# The secret values in this one are the same across all environments secret.deploy.config1: $(MAKE) secret.deploy name=envs/config1-secrets filename=config1.json # service2 has different secret values on the different environments secret.deploy.service2: $(MAKE) secret.deploy name=envs/$(environment)/service2-secrets filename=service2.json # service 3 also has environment-specific secret values secret.deploy.service3: $(MAKE) secret.deploy name=envs/$(environment)/service3-secrets filename=service3.json Now we can tie these together into one rule for a whole environment:\n# Deploy all secrets for one environment secrets.deploy.all: $(MAKE) secret.deploy.config1 $(MAKE) secret.deploy.service2 $(MAKE) secret.deploy.service3 # Deploy all secrets for the dev cluster secrets.deploy.dev: $(MAKE) secrets.deploy.all environment=dev $(MAKE) secrets.deploy.all environment=staging # Deploy all secrets for the prod cluster secrets.deploy.prod: $(MAKE) secrets.deploy.all environment=prod ✅ Once you are authenticated to the corresponding AWS account, you can deploy your secrets with either make secrets.deploy.dev or make secrets.deploy.prod.\nSummarizing  Make gives you a consistent and clean Developer API. Make is almost universally installed everywhere. Don\u0026rsquo;t decide between either Make or Shell - use both together. Refactor more complex logic into separate Shell scripts (like isolated functions) which are called from within Make.  You can check out the entire Makefile (including the secrets structure and scripts) in this Git repo.\nCredits I am indebted to the following parties in making my start into the world of Make a lot smoother than I expected:\n Isaac Z. Schlueter for his interactive Gist The guys at Upbound for creating Crossplane where they use Make in their providers and even distribute common functionality as a Git submodule  ","date":"2021-09-06T13:38:15+02:00","permalink":"https://jscheytt.github.io/p/the-story-of-my-first-makefile-half-versioned-secrets-management/","title":"The Story of My First Makefile — Half-Versioned Secrets Management"},{"content":"HTML allows you to specify a starting number from which an ordered list (\u0026lt;ol\u0026gt;) should start. I thought \u0026ldquo;Is there an upper bound?\u0026rdquo;.\nTurns out: Yes, there is. It\u0026rsquo;s 2147483647 (i.e. 231-1 ). Looks like a signed 32-bit integer to me.\n\u0026lt;ol start=2147483645\u0026gt; \u0026lt;li\u0026gt;I am still in order\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;As am I\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Me too\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;💥 Limit reached\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;💥 Limit reached\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;💥 Limit reached\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; The above snippet will render as the following:\n I am still in order As am I Me too 💥 Limit reached 💥 Limit reached 💥 Limit reached   ","date":"2021-09-02T08:55:22+02:00","permalink":"https://jscheytt.github.io/p/html-limits-you-to-a-signed-32-bit-int-in-ordered-lists/","title":"HTML limits you to a signed 32-bit int in ordered lists"},{"content":"All you need is the secret of your TOTP.\nThe QR code is just a representation of a oath:// URL That URL contains the secret as a query parameter.\n# Install oathtool brew install oathtool # Use your secret, e.g. as a base32-encoded string oathtool --totp --base32 \u0026#34;MFRGCZDTMVRXEZLUBI======\u0026#34; ","date":"2021-08-27T08:20:41+02:00","permalink":"https://jscheytt.github.io/p/you-can-create-totp-tokens-via-cli-without-a-smartphone/","title":"You can create TOTP tokens via CLI without a smartphone"},{"content":"I really love Docker, and I also come to like security more and more. One advice I have been hearing a lot (e.g. in this Container Security Cheat Sheet from Snyk) is that you should not run your container as a root user.\n\u0026ldquo;Easy thing,\u0026rdquo; I thought to myself, \u0026ldquo;I am just going to put something like USER {app}\u0026rdquo; at the top of my Dockerfile.\u0026quot; Well, think again, because:\nFROMnode:ltsUSERnode# I would have thought that after this point, every action will happen in the name of this user# and also that every created directory and file will belong to this user ... 😕 But:# ⚠️ This directory is created by root:root!WORKDIR/app# ⚠️ These files will be copied over to be owned by root:root!COPY package*.json ./# 💥 This step fails in some (not all!) environments with errors like \u0026#34;Not enough permissions on /app\u0026#34;RUN npm install \u0026amp;\u0026amp; \\  npm run verify# ⚠️ If you manage to get to this point, these files, too, will be copied over to be owned by root:root!COPY . .ENTRYPOINT [\u0026#34;npm\u0026#34;]I ended up fixing it by creating the directory and then chown-ing it. Equally I executed the COPY instructions with the --chown flag. In the end I refactored it a bit using some ENVs, too:\nFROMnode:lts# Ensure that target WORKDIR exists and is owned by target (non-root) userENV USERNAME=node ENV USERID=$USERNAME:$USERNAMEENV TARGETDIR=/appRUN mkdir -p $TARGETDIR \u0026amp;\u0026amp; chown -R $USERID $TARGETDIRWORKDIR$TARGETDIRUSER$USERNAMECOPY --chown=$USERID package*.json ./RUN npm install \u0026amp;\u0026amp; \\  npm run verifyCOPY --chown=$USERID . .ENTRYPOINT [\u0026#34;npm\u0026#34;]","date":"2021-07-23T14:42:07+02:00","permalink":"https://jscheytt.github.io/p/docker-does-not-really-help-you-a-lot-trying-to-get-permissions-right/","title":"Docker does not really help you a lot trying to get permissions right"},{"content":"I have developed quite some muscle memory in terminating vim via :wqa. Now I can save one more character by retraining myself to use :xa! 😄\nI also like how :x sort of symbolizes the close button ❌ of many window managers and their default behavior of saving when you close a file.\nFor more documentation open vim and type :h :x.\nWait, now that you mention it \u0026hellip; Well, turns out the documentation mentions an even easier way: Just press ZZ (without the colon) 😴\nNow if only I could combine this to perform :wqa \u0026hellip;\nHappy vim-ing! 👋\n","date":"2021-07-16T00:00:00Z","permalink":"https://jscheytt.github.io/p/you-can-wq-in-vim-with-x/","title":"You can :wq in vim with :x"}]